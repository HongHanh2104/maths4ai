{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3927411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea91cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regression import linear_regression, normalize, lr_visualize_loss\n",
    "from linear_regression_pytorch import LinearRegression, train, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67d4f0",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebc7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb1f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42631bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece159fd",
   "metadata": {},
   "source": [
    "# Parameters Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41513c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "steps = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8620a",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a5e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(W, b, X, y):\n",
    "    X = normalize(X)\n",
    "    predict = X @ W + b\n",
    "    return 1 - (np.sum(((y - predict)**2))/np.sum((y - np.mean(y))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d75f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, loss_history = linear_regression(X_train, y_train, lr, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "530c2100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7730135531648108\n",
      "Test score: 0.5687366730857608\n"
     ]
    }
   ],
   "source": [
    "train_score = score(W, b, X_train, y_train)\n",
    "test_score = score(W, b, X_test, y_test)\n",
    "print(f'Train score: {train_score}')\n",
    "print(f'Test score: {test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9641e",
   "metadata": {},
   "source": [
    "# Linear Regression (pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c4110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(X, y):\n",
    "    datasets = TensorDataset(X, y)\n",
    "    return DataLoader(datasets, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4066aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_id = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(dev_id)\n",
    "\n",
    "X_train_ts = torch.Tensor(normalize(X_train)).to(device)\n",
    "y_train_ts = torch.Tensor(y_train).to(device)\n",
    "train_iter = prepare_dataset(X_train_ts, y_train_ts)\n",
    "\n",
    "model = LinearRegression(X_train_ts.shape[1])\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss(size_average = False).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561c89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 97898216.0\n",
      "Epoch 1, loss 52044836.0\n",
      "Epoch 2, loss 31240716.0\n",
      "Epoch 3, loss 21801412.0\n",
      "Epoch 4, loss 17518408.0\n",
      "Epoch 5, loss 15574892.0\n",
      "Epoch 6, loss 14692874.0\n",
      "Epoch 7, loss 14292509.0\n",
      "Epoch 8, loss 14110711.0\n",
      "Epoch 9, loss 14028108.0\n",
      "Epoch 10, loss 13990534.0\n",
      "Epoch 11, loss 13973408.0\n",
      "Epoch 12, loss 13965576.0\n",
      "Epoch 13, loss 13961976.0\n",
      "Epoch 14, loss 13960304.0\n",
      "Epoch 15, loss 13959517.0\n",
      "Epoch 16, loss 13959140.0\n",
      "Epoch 17, loss 13958954.0\n",
      "Epoch 18, loss 13958862.0\n",
      "Epoch 19, loss 13958818.0\n",
      "Epoch 20, loss 13958800.0\n",
      "Epoch 21, loss 13958796.0\n",
      "Epoch 22, loss 13958805.0\n",
      "Epoch 23, loss 13958820.0\n",
      "Epoch 24, loss 13958841.0\n",
      "Epoch 25, loss 13958866.0\n",
      "Epoch 26, loss 13958896.0\n",
      "Epoch 27, loss 13958932.0\n",
      "Epoch 28, loss 13958970.0\n",
      "Epoch 29, loss 13959010.0\n",
      "Epoch 30, loss 13959052.0\n",
      "Epoch 31, loss 13959097.0\n",
      "Epoch 32, loss 13959145.0\n",
      "Epoch 33, loss 13959194.0\n",
      "Epoch 34, loss 13959246.0\n",
      "Epoch 35, loss 13959296.0\n",
      "Epoch 36, loss 13959350.0\n",
      "Epoch 37, loss 13959404.0\n",
      "Epoch 38, loss 13959460.0\n",
      "Epoch 39, loss 13959516.0\n",
      "Epoch 40, loss 13959574.0\n",
      "Epoch 41, loss 13959631.0\n",
      "Epoch 42, loss 13959689.0\n",
      "Epoch 43, loss 13959748.0\n",
      "Epoch 44, loss 13959808.0\n",
      "Epoch 45, loss 13959868.0\n",
      "Epoch 46, loss 13959928.0\n",
      "Epoch 47, loss 13959990.0\n",
      "Epoch 48, loss 13960052.0\n",
      "Epoch 49, loss 13960114.0\n",
      "Epoch 50, loss 13960176.0\n",
      "Epoch 51, loss 13960238.0\n",
      "Epoch 52, loss 13960302.0\n",
      "Epoch 53, loss 13960364.0\n",
      "Epoch 54, loss 13960428.0\n",
      "Epoch 55, loss 13960490.0\n",
      "Epoch 56, loss 13960555.0\n",
      "Epoch 57, loss 13960618.0\n",
      "Epoch 58, loss 13960682.0\n",
      "Epoch 59, loss 13960746.0\n",
      "Epoch 60, loss 13960811.0\n",
      "Epoch 61, loss 13960876.0\n",
      "Epoch 62, loss 13960940.0\n",
      "Epoch 63, loss 13961006.0\n",
      "Epoch 64, loss 13961070.0\n",
      "Epoch 65, loss 13961135.0\n",
      "Epoch 66, loss 13961201.0\n",
      "Epoch 67, loss 13961265.0\n",
      "Epoch 68, loss 13961332.0\n",
      "Epoch 69, loss 13961396.0\n",
      "Epoch 70, loss 13961461.0\n",
      "Epoch 71, loss 13961528.0\n",
      "Epoch 72, loss 13961593.0\n",
      "Epoch 73, loss 13961660.0\n",
      "Epoch 74, loss 13961724.0\n",
      "Epoch 75, loss 13961792.0\n",
      "Epoch 76, loss 13961858.0\n",
      "Epoch 77, loss 13961922.0\n",
      "Epoch 78, loss 13961991.0\n",
      "Epoch 79, loss 13962056.0\n",
      "Epoch 80, loss 13962122.0\n",
      "Epoch 81, loss 13962190.0\n",
      "Epoch 82, loss 13962256.0\n",
      "Epoch 83, loss 13962322.0\n",
      "Epoch 84, loss 13962390.0\n",
      "Epoch 85, loss 13962458.0\n",
      "Epoch 86, loss 13962524.0\n",
      "Epoch 87, loss 13962590.0\n",
      "Epoch 88, loss 13962658.0\n",
      "Epoch 89, loss 13962725.0\n",
      "Epoch 90, loss 13962792.0\n",
      "Epoch 91, loss 13962860.0\n",
      "Epoch 92, loss 13962928.0\n",
      "Epoch 93, loss 13962994.0\n",
      "Epoch 94, loss 13963062.0\n",
      "Epoch 95, loss 13963130.0\n",
      "Epoch 96, loss 13963198.0\n",
      "Epoch 97, loss 13963266.0\n",
      "Epoch 98, loss 13963334.0\n",
      "Epoch 99, loss 13963402.0\n",
      "Epoch 100, loss 13963470.0\n",
      "Epoch 101, loss 13963538.0\n",
      "Epoch 102, loss 13963606.0\n",
      "Epoch 103, loss 13963675.0\n",
      "Epoch 104, loss 13963743.0\n",
      "Epoch 105, loss 13963812.0\n",
      "Epoch 106, loss 13963880.0\n",
      "Epoch 107, loss 13963948.0\n",
      "Epoch 108, loss 13964018.0\n",
      "Epoch 109, loss 13964086.0\n",
      "Epoch 110, loss 13964154.0\n",
      "Epoch 111, loss 13964224.0\n",
      "Epoch 112, loss 13964292.0\n",
      "Epoch 113, loss 13964362.0\n",
      "Epoch 114, loss 13964431.0\n",
      "Epoch 115, loss 13964500.0\n",
      "Epoch 116, loss 13964569.0\n",
      "Epoch 117, loss 13964638.0\n",
      "Epoch 118, loss 13964708.0\n",
      "Epoch 119, loss 13964778.0\n",
      "Epoch 120, loss 13964846.0\n",
      "Epoch 121, loss 13964916.0\n",
      "Epoch 122, loss 13964985.0\n",
      "Epoch 123, loss 13965055.0\n",
      "Epoch 124, loss 13965125.0\n",
      "Epoch 125, loss 13965196.0\n",
      "Epoch 126, loss 13965266.0\n",
      "Epoch 127, loss 13965335.0\n",
      "Epoch 128, loss 13965406.0\n",
      "Epoch 129, loss 13965476.0\n",
      "Epoch 130, loss 13965546.0\n",
      "Epoch 131, loss 13965616.0\n",
      "Epoch 132, loss 13965686.0\n",
      "Epoch 133, loss 13965757.0\n",
      "Epoch 134, loss 13965828.0\n",
      "Epoch 135, loss 13965898.0\n",
      "Epoch 136, loss 13965970.0\n",
      "Epoch 137, loss 13966039.0\n",
      "Epoch 138, loss 13966110.0\n",
      "Epoch 139, loss 13966181.0\n",
      "Epoch 140, loss 13966252.0\n",
      "Epoch 141, loss 13966324.0\n",
      "Epoch 142, loss 13966395.0\n",
      "Epoch 143, loss 13966466.0\n",
      "Epoch 144, loss 13966537.0\n",
      "Epoch 145, loss 13966609.0\n",
      "Epoch 146, loss 13966680.0\n",
      "Epoch 147, loss 13966752.0\n",
      "Epoch 148, loss 13966824.0\n",
      "Epoch 149, loss 13966896.0\n",
      "Epoch 150, loss 13966966.0\n",
      "Epoch 151, loss 13967040.0\n",
      "Epoch 152, loss 13967110.0\n",
      "Epoch 153, loss 13967182.0\n",
      "Epoch 154, loss 13967254.0\n",
      "Epoch 155, loss 13967326.0\n",
      "Epoch 156, loss 13967398.0\n",
      "Epoch 157, loss 13967470.0\n",
      "Epoch 158, loss 13967542.0\n",
      "Epoch 159, loss 13967616.0\n",
      "Epoch 160, loss 13967687.0\n",
      "Epoch 161, loss 13967760.0\n",
      "Epoch 162, loss 13967833.0\n",
      "Epoch 163, loss 13967904.0\n",
      "Epoch 164, loss 13967978.0\n",
      "Epoch 165, loss 13968051.0\n",
      "Epoch 166, loss 13968124.0\n",
      "Epoch 167, loss 13968198.0\n",
      "Epoch 168, loss 13968270.0\n",
      "Epoch 169, loss 13968344.0\n",
      "Epoch 170, loss 13968417.0\n",
      "Epoch 171, loss 13968490.0\n",
      "Epoch 172, loss 13968564.0\n",
      "Epoch 173, loss 13968636.0\n",
      "Epoch 174, loss 13968710.0\n",
      "Epoch 175, loss 13968784.0\n",
      "Epoch 176, loss 13968858.0\n",
      "Epoch 177, loss 13968930.0\n",
      "Epoch 178, loss 13969004.0\n",
      "Epoch 179, loss 13969078.0\n",
      "Epoch 180, loss 13969152.0\n",
      "Epoch 181, loss 13969227.0\n",
      "Epoch 182, loss 13969300.0\n",
      "Epoch 183, loss 13969374.0\n",
      "Epoch 184, loss 13969449.0\n",
      "Epoch 185, loss 13969523.0\n",
      "Epoch 186, loss 13969598.0\n",
      "Epoch 187, loss 13969673.0\n",
      "Epoch 188, loss 13969747.0\n",
      "Epoch 189, loss 13969822.0\n",
      "Epoch 190, loss 13969897.0\n",
      "Epoch 191, loss 13969972.0\n",
      "Epoch 192, loss 13970046.0\n",
      "Epoch 193, loss 13970121.0\n",
      "Epoch 194, loss 13970195.0\n",
      "Epoch 195, loss 13970272.0\n",
      "Epoch 196, loss 13970346.0\n",
      "Epoch 197, loss 13970422.0\n",
      "Epoch 198, loss 13970496.0\n",
      "Epoch 199, loss 13970572.0\n",
      "Epoch 200, loss 13970647.0\n",
      "Epoch 201, loss 13970724.0\n",
      "Epoch 202, loss 13970800.0\n",
      "Epoch 203, loss 13970874.0\n",
      "Epoch 204, loss 13970950.0\n",
      "Epoch 205, loss 13971026.0\n",
      "Epoch 206, loss 13971102.0\n",
      "Epoch 207, loss 13971178.0\n",
      "Epoch 208, loss 13971254.0\n",
      "Epoch 209, loss 13971330.0\n",
      "Epoch 210, loss 13971408.0\n",
      "Epoch 211, loss 13971484.0\n",
      "Epoch 212, loss 13971560.0\n",
      "Epoch 213, loss 13971636.0\n",
      "Epoch 214, loss 13971713.0\n",
      "Epoch 215, loss 13971790.0\n",
      "Epoch 216, loss 13971866.0\n",
      "Epoch 217, loss 13971943.0\n",
      "Epoch 218, loss 13972019.0\n",
      "Epoch 219, loss 13972097.0\n",
      "Epoch 220, loss 13972174.0\n",
      "Epoch 221, loss 13972250.0\n",
      "Epoch 222, loss 13972328.0\n",
      "Epoch 223, loss 13972406.0\n",
      "Epoch 224, loss 13972482.0\n",
      "Epoch 225, loss 13972560.0\n",
      "Epoch 226, loss 13972637.0\n",
      "Epoch 227, loss 13972716.0\n",
      "Epoch 228, loss 13972794.0\n",
      "Epoch 229, loss 13972871.0\n",
      "Epoch 230, loss 13972948.0\n",
      "Epoch 231, loss 13973027.0\n",
      "Epoch 232, loss 13973104.0\n",
      "Epoch 233, loss 13973182.0\n",
      "Epoch 234, loss 13973261.0\n",
      "Epoch 235, loss 13973340.0\n",
      "Epoch 236, loss 13973418.0\n",
      "Epoch 237, loss 13973496.0\n",
      "Epoch 238, loss 13973574.0\n",
      "Epoch 239, loss 13973654.0\n",
      "Epoch 240, loss 13973730.0\n",
      "Epoch 241, loss 13973811.0\n",
      "Epoch 242, loss 13973889.0\n",
      "Epoch 243, loss 13973968.0\n",
      "Epoch 244, loss 13974048.0\n",
      "Epoch 245, loss 13974126.0\n",
      "Epoch 246, loss 13974206.0\n",
      "Epoch 247, loss 13974284.0\n",
      "Epoch 248, loss 13974364.0\n",
      "Epoch 249, loss 13974442.0\n",
      "Epoch 250, loss 13974522.0\n",
      "Epoch 251, loss 13974602.0\n",
      "Epoch 252, loss 13974681.0\n",
      "Epoch 253, loss 13974761.0\n",
      "Epoch 254, loss 13974841.0\n",
      "Epoch 255, loss 13974922.0\n",
      "Epoch 256, loss 13975001.0\n",
      "Epoch 257, loss 13975080.0\n",
      "Epoch 258, loss 13975160.0\n",
      "Epoch 259, loss 13975242.0\n",
      "Epoch 260, loss 13975322.0\n",
      "Epoch 261, loss 13975402.0\n",
      "Epoch 262, loss 13975482.0\n",
      "Epoch 263, loss 13975562.0\n",
      "Epoch 264, loss 13975644.0\n",
      "Epoch 265, loss 13975724.0\n",
      "Epoch 266, loss 13975805.0\n",
      "Epoch 267, loss 13975886.0\n",
      "Epoch 268, loss 13975966.0\n",
      "Epoch 269, loss 13976048.0\n",
      "Epoch 270, loss 13976128.0\n",
      "Epoch 271, loss 13976210.0\n",
      "Epoch 272, loss 13976292.0\n",
      "Epoch 273, loss 13976372.0\n",
      "Epoch 274, loss 13976454.0\n",
      "Epoch 275, loss 13976535.0\n",
      "Epoch 276, loss 13976618.0\n",
      "Epoch 277, loss 13976698.0\n",
      "Epoch 278, loss 13976780.0\n",
      "Epoch 279, loss 13976862.0\n",
      "Epoch 280, loss 13976944.0\n",
      "Epoch 281, loss 13977026.0\n",
      "Epoch 282, loss 13977108.0\n",
      "Epoch 283, loss 13977192.0\n",
      "Epoch 284, loss 13977273.0\n",
      "Epoch 285, loss 13977355.0\n",
      "Epoch 286, loss 13977437.0\n",
      "Epoch 287, loss 13977520.0\n",
      "Epoch 288, loss 13977603.0\n",
      "Epoch 289, loss 13977686.0\n",
      "Epoch 290, loss 13977768.0\n",
      "Epoch 291, loss 13977852.0\n",
      "Epoch 292, loss 13977934.0\n",
      "Epoch 293, loss 13978017.0\n",
      "Epoch 294, loss 13978100.0\n",
      "Epoch 295, loss 13978183.0\n",
      "Epoch 296, loss 13978268.0\n",
      "Epoch 297, loss 13978350.0\n",
      "Epoch 298, loss 13978434.0\n",
      "Epoch 299, loss 13978518.0\n",
      "Epoch 300, loss 13978600.0\n",
      "Epoch 301, loss 13978684.0\n",
      "Epoch 302, loss 13978768.0\n",
      "Epoch 303, loss 13978852.0\n",
      "Epoch 304, loss 13978936.0\n",
      "Epoch 305, loss 13979020.0\n",
      "Epoch 306, loss 13979105.0\n",
      "Epoch 307, loss 13979188.0\n",
      "Epoch 308, loss 13979274.0\n",
      "Epoch 309, loss 13979358.0\n",
      "Epoch 310, loss 13979441.0\n",
      "Epoch 311, loss 13979526.0\n",
      "Epoch 312, loss 13979610.0\n",
      "Epoch 313, loss 13979696.0\n",
      "Epoch 314, loss 13979780.0\n",
      "Epoch 315, loss 13979866.0\n",
      "Epoch 316, loss 13979950.0\n",
      "Epoch 317, loss 13980035.0\n",
      "Epoch 318, loss 13980120.0\n",
      "Epoch 319, loss 13980207.0\n",
      "Epoch 320, loss 13980291.0\n",
      "Epoch 321, loss 13980376.0\n",
      "Epoch 322, loss 13980464.0\n",
      "Epoch 323, loss 13980548.0\n",
      "Epoch 324, loss 13980633.0\n",
      "Epoch 325, loss 13980720.0\n",
      "Epoch 326, loss 13980806.0\n",
      "Epoch 327, loss 13980890.0\n",
      "Epoch 328, loss 13980976.0\n",
      "Epoch 329, loss 13981063.0\n",
      "Epoch 330, loss 13981148.0\n",
      "Epoch 331, loss 13981236.0\n",
      "Epoch 332, loss 13981322.0\n",
      "Epoch 333, loss 13981410.0\n",
      "Epoch 334, loss 13981495.0\n",
      "Epoch 335, loss 13981582.0\n",
      "Epoch 336, loss 13981668.0\n",
      "Epoch 337, loss 13981756.0\n",
      "Epoch 338, loss 13981842.0\n",
      "Epoch 339, loss 13981930.0\n",
      "Epoch 340, loss 13982016.0\n",
      "Epoch 341, loss 13982104.0\n",
      "Epoch 342, loss 13982190.0\n",
      "Epoch 343, loss 13982280.0\n",
      "Epoch 344, loss 13982366.0\n",
      "Epoch 345, loss 13982453.0\n",
      "Epoch 346, loss 13982542.0\n",
      "Epoch 347, loss 13982628.0\n",
      "Epoch 348, loss 13982717.0\n",
      "Epoch 349, loss 13982805.0\n",
      "Epoch 350, loss 13982892.0\n",
      "Epoch 351, loss 13982980.0\n",
      "Epoch 352, loss 13983070.0\n",
      "Epoch 353, loss 13983158.0\n",
      "Epoch 354, loss 13983246.0\n",
      "Epoch 355, loss 13983334.0\n",
      "Epoch 356, loss 13983422.0\n",
      "Epoch 357, loss 13983512.0\n",
      "Epoch 358, loss 13983600.0\n",
      "Epoch 359, loss 13983690.0\n",
      "Epoch 360, loss 13983778.0\n",
      "Epoch 361, loss 13983866.0\n",
      "Epoch 362, loss 13983955.0\n",
      "Epoch 363, loss 13984046.0\n",
      "Epoch 364, loss 13984134.0\n",
      "Epoch 365, loss 13984224.0\n",
      "Epoch 366, loss 13984313.0\n",
      "Epoch 367, loss 13984403.0\n",
      "Epoch 368, loss 13984493.0\n",
      "Epoch 369, loss 13984584.0\n",
      "Epoch 370, loss 13984672.0\n",
      "Epoch 371, loss 13984762.0\n",
      "Epoch 372, loss 13984852.0\n",
      "Epoch 373, loss 13984942.0\n",
      "Epoch 374, loss 13985033.0\n",
      "Epoch 375, loss 13985122.0\n",
      "Epoch 376, loss 13985212.0\n",
      "Epoch 377, loss 13985304.0\n",
      "Epoch 378, loss 13985394.0\n",
      "Epoch 379, loss 13985485.0\n",
      "Epoch 380, loss 13985575.0\n",
      "Epoch 381, loss 13985667.0\n",
      "Epoch 382, loss 13985758.0\n",
      "Epoch 383, loss 13985850.0\n",
      "Epoch 384, loss 13985940.0\n",
      "Epoch 385, loss 13986031.0\n",
      "Epoch 386, loss 13986122.0\n",
      "Epoch 387, loss 13986214.0\n",
      "Epoch 388, loss 13986305.0\n",
      "Epoch 389, loss 13986396.0\n",
      "Epoch 390, loss 13986488.0\n",
      "Epoch 391, loss 13986580.0\n",
      "Epoch 392, loss 13986672.0\n",
      "Epoch 393, loss 13986764.0\n",
      "Epoch 394, loss 13986856.0\n",
      "Epoch 395, loss 13986948.0\n",
      "Epoch 396, loss 13987040.0\n",
      "Epoch 397, loss 13987132.0\n",
      "Epoch 398, loss 13987226.0\n",
      "Epoch 399, loss 13987318.0\n",
      "Epoch 400, loss 13987410.0\n",
      "Epoch 401, loss 13987504.0\n",
      "Epoch 402, loss 13987596.0\n",
      "Epoch 403, loss 13987689.0\n",
      "Epoch 404, loss 13987782.0\n",
      "Epoch 405, loss 13987875.0\n",
      "Epoch 406, loss 13987968.0\n",
      "Epoch 407, loss 13988061.0\n",
      "Epoch 408, loss 13988154.0\n",
      "Epoch 409, loss 13988249.0\n",
      "Epoch 410, loss 13988342.0\n",
      "Epoch 411, loss 13988436.0\n",
      "Epoch 412, loss 13988529.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413, loss 13988624.0\n",
      "Epoch 414, loss 13988716.0\n",
      "Epoch 415, loss 13988810.0\n",
      "Epoch 416, loss 13988904.0\n",
      "Epoch 417, loss 13989000.0\n",
      "Epoch 418, loss 13989093.0\n",
      "Epoch 419, loss 13989188.0\n",
      "Epoch 420, loss 13989282.0\n",
      "Epoch 421, loss 13989378.0\n",
      "Epoch 422, loss 13989472.0\n",
      "Epoch 423, loss 13989566.0\n",
      "Epoch 424, loss 13989662.0\n",
      "Epoch 425, loss 13989758.0\n",
      "Epoch 426, loss 13989852.0\n",
      "Epoch 427, loss 13989948.0\n",
      "Epoch 428, loss 13990042.0\n",
      "Epoch 429, loss 13990138.0\n",
      "Epoch 430, loss 13990233.0\n",
      "Epoch 431, loss 13990329.0\n",
      "Epoch 432, loss 13990425.0\n",
      "Epoch 433, loss 13990520.0\n",
      "Epoch 434, loss 13990616.0\n",
      "Epoch 435, loss 13990713.0\n",
      "Epoch 436, loss 13990808.0\n",
      "Epoch 437, loss 13990904.0\n",
      "Epoch 438, loss 13991002.0\n",
      "Epoch 439, loss 13991098.0\n",
      "Epoch 440, loss 13991194.0\n",
      "Epoch 441, loss 13991291.0\n",
      "Epoch 442, loss 13991388.0\n",
      "Epoch 443, loss 13991484.0\n",
      "Epoch 444, loss 13991580.0\n",
      "Epoch 445, loss 13991679.0\n",
      "Epoch 446, loss 13991775.0\n",
      "Epoch 447, loss 13991872.0\n",
      "Epoch 448, loss 13991970.0\n",
      "Epoch 449, loss 13992066.0\n",
      "Epoch 450, loss 13992165.0\n",
      "Epoch 451, loss 13992262.0\n",
      "Epoch 452, loss 13992360.0\n",
      "Epoch 453, loss 13992458.0\n",
      "Epoch 454, loss 13992555.0\n",
      "Epoch 455, loss 13992654.0\n",
      "Epoch 456, loss 13992752.0\n",
      "Epoch 457, loss 13992850.0\n",
      "Epoch 458, loss 13992949.0\n",
      "Epoch 459, loss 13993046.0\n",
      "Epoch 460, loss 13993144.0\n",
      "Epoch 461, loss 13993243.0\n",
      "Epoch 462, loss 13993343.0\n",
      "Epoch 463, loss 13993441.0\n",
      "Epoch 464, loss 13993540.0\n",
      "Epoch 465, loss 13993639.0\n",
      "Epoch 466, loss 13993738.0\n",
      "Epoch 467, loss 13993838.0\n",
      "Epoch 468, loss 13993936.0\n",
      "Epoch 469, loss 13994036.0\n",
      "Epoch 470, loss 13994136.0\n",
      "Epoch 471, loss 13994235.0\n",
      "Epoch 472, loss 13994336.0\n",
      "Epoch 473, loss 13994435.0\n",
      "Epoch 474, loss 13994534.0\n",
      "Epoch 475, loss 13994634.0\n",
      "Epoch 476, loss 13994735.0\n",
      "Epoch 477, loss 13994834.0\n",
      "Epoch 478, loss 13994935.0\n",
      "Epoch 479, loss 13995036.0\n",
      "Epoch 480, loss 13995136.0\n",
      "Epoch 481, loss 13995238.0\n",
      "Epoch 482, loss 13995337.0\n",
      "Epoch 483, loss 13995438.0\n",
      "Epoch 484, loss 13995539.0\n",
      "Epoch 485, loss 13995640.0\n",
      "Epoch 486, loss 13995741.0\n",
      "Epoch 487, loss 13995843.0\n",
      "Epoch 488, loss 13995944.0\n",
      "Epoch 489, loss 13996045.0\n",
      "Epoch 490, loss 13996147.0\n",
      "Epoch 491, loss 13996249.0\n",
      "Epoch 492, loss 13996350.0\n",
      "Epoch 493, loss 13996452.0\n",
      "Epoch 494, loss 13996554.0\n",
      "Epoch 495, loss 13996656.0\n",
      "Epoch 496, loss 13996759.0\n",
      "Epoch 497, loss 13996861.0\n",
      "Epoch 498, loss 13996964.0\n",
      "Epoch 499, loss 13997066.0\n",
      "Complete training\n"
     ]
    }
   ],
   "source": [
    "trained_model = train((X_train_ts, y_train_ts), model, criterion, optimizer, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053a709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
